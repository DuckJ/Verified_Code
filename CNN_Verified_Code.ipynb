{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "471\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAEICAYAAACQ6CLfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nfrNMOfjNMuXgN8vU/wOobzIcqePEUwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7551dfca58>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1 (?, 40, 40, 16)\n",
      "pool1 (?, 20, 20, 16)\n",
      "conv2 (?, 20, 20, 32)\n",
      "pool2 (?, 10, 10, 32)\n",
      "con3 (?, 10, 10, 64)\n",
      "pool3 (?, 5, 5, 64)\n",
      "flat (?, 1600)\n",
      "output (?, 1024)\n",
      "output_drop (?, 1024)\n",
      "prediction (?, 26)\n",
      "Step: 0 | train loss: 3.2706 | test accuracy: 0.04\n",
      "Step: 50 | train loss: 2.3877 | test accuracy: 0.27\n",
      "Step: 100 | train loss: 0.9476 | test accuracy: 0.41\n",
      "Step: 150 | train loss: 0.5385 | test accuracy: 0.52\n",
      "Step: 200 | train loss: 0.7018 | test accuracy: 0.59\n",
      "Step: 250 | train loss: 0.3790 | test accuracy: 0.64\n",
      "Step: 300 | train loss: 0.4444 | test accuracy: 0.68\n",
      "Step: 350 | train loss: 0.2782 | test accuracy: 0.71\n",
      "Step: 400 | train loss: 0.2961 | test accuracy: 0.73\n",
      "Step: 450 | train loss: 0.1708 | test accuracy: 0.75\n",
      "Step: 500 | train loss: 0.1391 | test accuracy: 0.77\n",
      "Step: 550 | train loss: 0.1560 | test accuracy: 0.78\n",
      "Step: 600 | train loss: 0.2519 | test accuracy: 0.79\n",
      "Step: 650 | train loss: 0.1074 | test accuracy: 0.81\n",
      "Step: 700 | train loss: 0.0976 | test accuracy: 0.81\n",
      "Step: 750 | train loss: 0.1444 | test accuracy: 0.82\n",
      "Step: 800 | train loss: 0.0805 | test accuracy: 0.83\n",
      "Step: 850 | train loss: 0.0526 | test accuracy: 0.84\n",
      "Step: 900 | train loss: 0.0872 | test accuracy: 0.84\n",
      "Step: 950 | train loss: 0.1095 | test accuracy: 0.85\n",
      "Step: 1000 | train loss: 0.1628 | test accuracy: 0.85\n",
      "Step: 1050 | train loss: 0.0311 | test accuracy: 0.86\n",
      "Step: 1100 | train loss: 0.0362 | test accuracy: 0.86\n",
      "Step: 1150 | train loss: 0.0774 | test accuracy: 0.87\n",
      "Step: 1200 | train loss: 0.1128 | test accuracy: 0.87\n",
      "Step: 1250 | train loss: 0.0624 | test accuracy: 0.87\n",
      "Step: 1300 | train loss: 0.0596 | test accuracy: 0.88\n",
      "Step: 1350 | train loss: 0.0702 | test accuracy: 0.88\n",
      "Step: 1400 | train loss: 0.0453 | test accuracy: 0.88\n",
      "Step: 1450 | train loss: 0.0320 | test accuracy: 0.88\n",
      "Step: 1500 | train loss: 0.0621 | test accuracy: 0.89\n",
      "Step: 1550 | train loss: 0.1531 | test accuracy: 0.89\n",
      "Step: 1600 | train loss: 0.1075 | test accuracy: 0.89\n",
      "Step: 1650 | train loss: 0.0139 | test accuracy: 0.89\n",
      "Step: 1700 | train loss: 0.0250 | test accuracy: 0.89\n",
      "Step: 1750 | train loss: 0.0701 | test accuracy: 0.90\n",
      "Step: 1800 | train loss: 0.0612 | test accuracy: 0.90\n",
      "Step: 1850 | train loss: 0.0509 | test accuracy: 0.90\n",
      "Step: 1900 | train loss: 0.0447 | test accuracy: 0.90\n",
      "Step: 1950 | train loss: 0.0356 | test accuracy: 0.90\n",
      "Step: 2000 | train loss: 0.0336 | test accuracy: 0.90\n",
      "Step: 2050 | train loss: 0.0095 | test accuracy: 0.91\n",
      "Step: 2100 | train loss: 0.0497 | test accuracy: 0.91\n",
      "Step: 2150 | train loss: 0.0746 | test accuracy: 0.91\n",
      "Step: 2200 | train loss: 0.1124 | test accuracy: 0.91\n",
      "Step: 2250 | train loss: 0.0115 | test accuracy: 0.91\n",
      "Step: 2300 | train loss: 0.0271 | test accuracy: 0.91\n",
      "Step: 2350 | train loss: 0.0828 | test accuracy: 0.91\n",
      "Step: 2400 | train loss: 0.0423 | test accuracy: 0.91\n",
      "Step: 2450 | train loss: 0.0448 | test accuracy: 0.91\n",
      "Step: 2500 | train loss: 0.0431 | test accuracy: 0.92\n",
      "Step: 2550 | train loss: 0.0299 | test accuracy: 0.92\n",
      "Step: 2600 | train loss: 0.0433 | test accuracy: 0.92\n",
      "Step: 2650 | train loss: 0.0087 | test accuracy: 0.92\n",
      "Step: 2700 | train loss: 0.0463 | test accuracy: 0.92\n",
      "Step: 2750 | train loss: 0.0732 | test accuracy: 0.92\n",
      "Step: 2800 | train loss: 0.1194 | test accuracy: 0.92\n",
      "Step: 2850 | train loss: 0.0131 | test accuracy: 0.92\n",
      "Step: 2900 | train loss: 0.0230 | test accuracy: 0.92\n",
      "Step: 2950 | train loss: 0.0951 | test accuracy: 0.92\n",
      "Step: 3000 | train loss: 0.0342 | test accuracy: 0.92\n",
      "Step: 3050 | train loss: 0.0408 | test accuracy: 0.92\n",
      "Step: 3100 | train loss: 0.0356 | test accuracy: 0.92\n",
      "Step: 3150 | train loss: 0.0302 | test accuracy: 0.92\n",
      "Step: 3200 | train loss: 0.0468 | test accuracy: 0.93\n",
      "Step: 3250 | train loss: 0.0102 | test accuracy: 0.93\n",
      "Step: 3300 | train loss: 0.0361 | test accuracy: 0.93\n",
      "Step: 3350 | train loss: 0.0828 | test accuracy: 0.93\n",
      "Step: 3400 | train loss: 0.1277 | test accuracy: 0.93\n",
      "Step: 3450 | train loss: 0.0121 | test accuracy: 0.93\n",
      "Step: 3500 | train loss: 0.0217 | test accuracy: 0.93\n",
      "Step: 3550 | train loss: 0.0800 | test accuracy: 0.93\n",
      "Step: 3600 | train loss: 0.0262 | test accuracy: 0.93\n",
      "Step: 3650 | train loss: 0.0337 | test accuracy: 0.93\n",
      "Step: 3700 | train loss: 0.0308 | test accuracy: 0.93\n",
      "Step: 3750 | train loss: 0.0355 | test accuracy: 0.93\n",
      "Step: 3800 | train loss: 0.0398 | test accuracy: 0.93\n",
      "Step: 3850 | train loss: 0.0074 | test accuracy: 0.93\n",
      "Step: 3900 | train loss: 0.0326 | test accuracy: 0.93\n",
      "Step: 3950 | train loss: 0.0907 | test accuracy: 0.93\n",
      "Step: 4000 | train loss: 0.1180 | test accuracy: 0.93\n",
      "Step: 4050 | train loss: 0.0107 | test accuracy: 0.93\n",
      "Step: 4100 | train loss: 0.0281 | test accuracy: 0.93\n",
      "Step: 4150 | train loss: 0.0763 | test accuracy: 0.93\n",
      "Step: 4200 | train loss: 0.0276 | test accuracy: 0.93\n",
      "Step: 4250 | train loss: 0.0276 | test accuracy: 0.93\n",
      "Step: 4300 | train loss: 0.0285 | test accuracy: 0.93\n",
      "Step: 4350 | train loss: 0.0391 | test accuracy: 0.94\n",
      "Step: 4400 | train loss: 0.0345 | test accuracy: 0.94\n",
      "Step: 4450 | train loss: 0.0058 | test accuracy: 0.94\n",
      "Step: 4500 | train loss: 0.0361 | test accuracy: 0.94\n",
      "Step: 4550 | train loss: 0.0842 | test accuracy: 0.94\n",
      "Step: 4600 | train loss: 0.0999 | test accuracy: 0.94\n",
      "Step: 4650 | train loss: 0.0103 | test accuracy: 0.94\n",
      "Step: 4700 | train loss: 0.0333 | test accuracy: 0.94\n",
      "Step: 4750 | train loss: 0.0808 | test accuracy: 0.94\n",
      "Step: 4800 | train loss: 0.0290 | test accuracy: 0.94\n",
      "Step: 4850 | train loss: 0.0251 | test accuracy: 0.94\n",
      "Step: 4900 | train loss: 0.0268 | test accuracy: 0.94\n",
      "Step: 4950 | train loss: 0.0365 | test accuracy: 0.94\n",
      "Step: 5000 | train loss: 0.0307 | test accuracy: 0.94\n",
      "Step: 5050 | train loss: 0.0049 | test accuracy: 0.94\n",
      "Step: 5100 | train loss: 0.0422 | test accuracy: 0.94\n",
      "Step: 5150 | train loss: 0.0763 | test accuracy: 0.94\n",
      "Step: 5200 | train loss: 0.0892 | test accuracy: 0.94\n",
      "Step: 5250 | train loss: 0.0101 | test accuracy: 0.94\n",
      "Step: 5300 | train loss: 0.0360 | test accuracy: 0.94\n",
      "Step: 5350 | train loss: 0.0804 | test accuracy: 0.94\n",
      "Step: 5400 | train loss: 0.0304 | test accuracy: 0.94\n",
      "Step: 5450 | train loss: 0.0244 | test accuracy: 0.94\n",
      "Step: 5500 | train loss: 0.0246 | test accuracy: 0.94\n",
      "Step: 5550 | train loss: 0.0317 | test accuracy: 0.94\n",
      "Step: 5600 | train loss: 0.0274 | test accuracy: 0.94\n",
      "Step: 5650 | train loss: 0.0041 | test accuracy: 0.94\n",
      "Step: 5700 | train loss: 0.0515 | test accuracy: 0.94\n",
      "Step: 5750 | train loss: 0.0697 | test accuracy: 0.94\n",
      "Step: 5800 | train loss: 0.0848 | test accuracy: 0.94\n",
      "Step: 5850 | train loss: 0.0090 | test accuracy: 0.94\n",
      "Step: 5900 | train loss: 0.0379 | test accuracy: 0.94\n",
      "Step: 5950 | train loss: 0.0742 | test accuracy: 0.94\n",
      "Step: 6000 | train loss: 0.0309 | test accuracy: 0.94\n",
      "Step: 6050 | train loss: 0.0262 | test accuracy: 0.94\n",
      "Step: 6100 | train loss: 0.0229 | test accuracy: 0.94\n",
      "Step: 6150 | train loss: 0.0288 | test accuracy: 0.94\n",
      "Step: 6200 | train loss: 0.0256 | test accuracy: 0.94\n",
      "Step: 6250 | train loss: 0.0034 | test accuracy: 0.94\n",
      "Step: 6300 | train loss: 0.0573 | test accuracy: 0.94\n",
      "Step: 6350 | train loss: 0.0667 | test accuracy: 0.94\n",
      "Step: 6400 | train loss: 0.0823 | test accuracy: 0.94\n",
      "Step: 6450 | train loss: 0.0081 | test accuracy: 0.94\n",
      "Step: 6500 | train loss: 0.0383 | test accuracy: 0.94\n",
      "Step: 6550 | train loss: 0.0672 | test accuracy: 0.94\n",
      "Step: 6600 | train loss: 0.0301 | test accuracy: 0.94\n",
      "Step: 6650 | train loss: 0.0284 | test accuracy: 0.95\n",
      "Step: 6700 | train loss: 0.0215 | test accuracy: 0.95\n",
      "Step: 6750 | train loss: 0.0279 | test accuracy: 0.95\n",
      "Step: 6800 | train loss: 0.0242 | test accuracy: 0.95\n",
      "Step: 6850 | train loss: 0.0031 | test accuracy: 0.95\n",
      "Step: 6900 | train loss: 0.0581 | test accuracy: 0.95\n",
      "Step: 6950 | train loss: 0.0650 | test accuracy: 0.95\n",
      "Step: 7000 | train loss: 0.0796 | test accuracy: 0.95\n",
      "Step: 7050 | train loss: 0.0078 | test accuracy: 0.95\n",
      "Step: 7100 | train loss: 0.0397 | test accuracy: 0.95\n",
      "Step: 7150 | train loss: 0.0596 | test accuracy: 0.95\n",
      "Step: 7200 | train loss: 0.0296 | test accuracy: 0.95\n",
      "Step: 7250 | train loss: 0.0302 | test accuracy: 0.95\n",
      "Step: 7300 | train loss: 0.0207 | test accuracy: 0.95\n",
      "Step: 7350 | train loss: 0.0274 | test accuracy: 0.95\n",
      "Step: 7400 | train loss: 0.0239 | test accuracy: 0.95\n",
      "Step: 7450 | train loss: 0.0030 | test accuracy: 0.95\n",
      "Step: 7500 | train loss: 0.0553 | test accuracy: 0.95\n",
      "Step: 7550 | train loss: 0.0639 | test accuracy: 0.95\n",
      "Step: 7600 | train loss: 0.0780 | test accuracy: 0.95\n",
      "Step: 7650 | train loss: 0.0079 | test accuracy: 0.95\n",
      "Step: 7700 | train loss: 0.0407 | test accuracy: 0.95\n",
      "Step: 7750 | train loss: 0.0545 | test accuracy: 0.95\n",
      "Step: 7800 | train loss: 0.0287 | test accuracy: 0.95\n",
      "Step: 7850 | train loss: 0.0315 | test accuracy: 0.95\n",
      "Step: 7900 | train loss: 0.0204 | test accuracy: 0.95\n",
      "Step: 7950 | train loss: 0.0274 | test accuracy: 0.95\n",
      "Step: 8000 | train loss: 0.0230 | test accuracy: 0.95\n",
      "Step: 8050 | train loss: 0.0030 | test accuracy: 0.95\n",
      "Step: 8100 | train loss: 0.0534 | test accuracy: 0.95\n",
      "Step: 8150 | train loss: 0.0636 | test accuracy: 0.95\n",
      "Step: 8200 | train loss: 0.0753 | test accuracy: 0.95\n",
      "Step: 8250 | train loss: 0.0082 | test accuracy: 0.95\n",
      "Step: 8300 | train loss: 0.0413 | test accuracy: 0.95\n",
      "Step: 8350 | train loss: 0.0495 | test accuracy: 0.95\n",
      "Step: 8400 | train loss: 0.0290 | test accuracy: 0.95\n",
      "Step: 8450 | train loss: 0.0327 | test accuracy: 0.95\n",
      "Step: 8500 | train loss: 0.0209 | test accuracy: 0.95\n",
      "Step: 8550 | train loss: 0.0258 | test accuracy: 0.95\n",
      "Step: 8600 | train loss: 0.0222 | test accuracy: 0.95\n",
      "Step: 8650 | train loss: 0.0029 | test accuracy: 0.95\n",
      "Step: 8700 | train loss: 0.0504 | test accuracy: 0.95\n",
      "Step: 8750 | train loss: 0.0644 | test accuracy: 0.95\n",
      "Step: 8800 | train loss: 0.0721 | test accuracy: 0.95\n",
      "Step: 8850 | train loss: 0.0092 | test accuracy: 0.95\n",
      "Step: 8900 | train loss: 0.0433 | test accuracy: 0.95\n",
      "Step: 8950 | train loss: 0.0463 | test accuracy: 0.95\n",
      "Step: 9000 | train loss: 0.0288 | test accuracy: 0.95\n",
      "Step: 9050 | train loss: 0.0331 | test accuracy: 0.95\n",
      "Step: 9100 | train loss: 0.0208 | test accuracy: 0.95\n",
      "Step: 9150 | train loss: 0.0253 | test accuracy: 0.95\n",
      "Step: 9200 | train loss: 0.0209 | test accuracy: 0.95\n",
      "Step: 9250 | train loss: 0.0027 | test accuracy: 0.95\n",
      "Step: 9300 | train loss: 0.0482 | test accuracy: 0.95\n",
      "Step: 9350 | train loss: 0.0649 | test accuracy: 0.95\n",
      "Step: 9400 | train loss: 0.0717 | test accuracy: 0.95\n",
      "Step: 9450 | train loss: 0.0101 | test accuracy: 0.95\n",
      "Step: 9500 | train loss: 0.0430 | test accuracy: 0.95\n",
      "Step: 9550 | train loss: 0.0456 | test accuracy: 0.95\n",
      "Step: 9600 | train loss: 0.0285 | test accuracy: 0.95\n",
      "Step: 9650 | train loss: 0.0330 | test accuracy: 0.95\n",
      "Step: 9700 | train loss: 0.0224 | test accuracy: 0.95\n",
      "Step: 9750 | train loss: 0.0249 | test accuracy: 0.95\n",
      "Step: 9800 | train loss: 0.0202 | test accuracy: 0.95\n",
      "Step: 9850 | train loss: 0.0026 | test accuracy: 0.95\n",
      "Step: 9900 | train loss: 0.0452 | test accuracy: 0.95\n",
      "Step: 9950 | train loss: 0.0662 | test accuracy: 0.95\n",
      "test_output (20, 26)\n",
      "pred_y (20,)\n",
      "[14 14 21 14  4  0 20 19 10 23  0 22 18  1  0 14  2 12 17 17] prediction number\n",
      "[14 14 21 14  4  0 20 19 10 23  0 22 18  1  0 14  2 12 17 17] real number\n",
      "Model saved in file:  my_net/VerfiedCode_net.ckpt\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python\n",
    "#coding:utf-8\n",
    "import tensorflow as tf\n",
    "import Get_Database\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "trainset_path = \"/home/nfd/tensorflow/project/Valified_Code_Classify/Dataset/train_set1\"\n",
    "testset_path = \"/home/nfd/tensorflow/project/Valified_Code_Classify/Dataset/test\"\n",
    "data = Get_Database.Data(trainset_path, testset_path)\n",
    "\n",
    "BATCH_SIZE = 100\n",
    "LR = 0.0001              # learning rate\n",
    "\n",
    "\n",
    "# plot one example\n",
    "print(len(data.test_images))    # \n",
    "# print(data.test_labels)   #\n",
    "plt.imshow(data.test_images[0].reshape((40, 40)), cmap='gray')\n",
    "plt.title('%i' % np.argmax(data.test_labels[0]))\n",
    "plt.show()\n",
    "\n",
    "tf_x = tf.placeholder(tf.float32, [None, 40*40]) / 255.\n",
    "image = tf.reshape(tf_x, [-1, 40, 40, 1])              # (batch, height, width, channel)\n",
    "tf_y = tf.placeholder(tf.int32, [None, 26])            # input y\n",
    "rate = tf.placeholder(tf.float32)\n",
    "\n",
    "# def weight_variable(shape):\n",
    "#     initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "#     return tf.Variable(initial)\n",
    "\n",
    "# def bias_variable(shape):\n",
    "#     initial = tf.constant(0.1, shape=shape)\n",
    "#     return tf.Variable(initial)\n",
    "# CNN\n",
    "conv1 = tf.layers.conv2d(   # shape (40, 40, 1)\n",
    "    inputs=image,\n",
    "    filters=16,\n",
    "    kernel_size=5,\n",
    "    strides=1,\n",
    "    padding='same',\n",
    "    activation=tf.nn.relu\n",
    ")           # -> (40, 40, 16)\n",
    "print('conv1', conv1.shape)\n",
    "pool1 = tf.layers.max_pooling2d(\n",
    "    conv1,\n",
    "    pool_size=2,\n",
    "    strides=2,\n",
    ")           # -> (20, 20, 16)\n",
    "print('pool1', pool1.shape)\n",
    "conv2 = tf.layers.conv2d(pool1, 32, 5, 1, 'same', activation=tf.nn.relu)    # -> (20, 20, 32)\n",
    "print('conv2', conv2.shape)\n",
    "pool2 = tf.layers.max_pooling2d(conv2, 2, 2)    # -> (10, 10, 32)\n",
    "print('pool2', pool2.shape)\n",
    "\n",
    "conv3 = tf.layers.conv2d(pool2, 64, 5, 1, 'same', activation=tf.nn.relu)    # -> (10, 10, 64)\n",
    "print('con3', conv3.shape)\n",
    "pool3 = tf.layers.max_pooling2d(conv3, 2, 2)    # -> (5, 5, 64)\n",
    "print('pool3', pool3.shape)\n",
    "\n",
    "flat = tf.reshape(pool3, [-1, 5*5*64])          # -> (5*5*32, )\n",
    "print('flat', flat.shape)\n",
    "output = tf.layers.dense(flat, 1024) # output layer\n",
    "print('output', output.shape)\n",
    "output_drop = tf.layers.dropout(output, rate)\n",
    "print('output_drop', output.shape)\n",
    "\n",
    "\n",
    "prediction = tf.layers.dense(output_drop,26)\n",
    "print('prediction', prediction.shape)\n",
    "# prediction = tf.nn.softmax(tf.matmul(output_drop, W_fc2) + b_fc2)\n",
    "\n",
    "loss = tf.losses.softmax_cross_entropy(onehot_labels=tf_y, logits=prediction)           # compute cost\n",
    "train_op = tf.train.AdamOptimizer(LR).minimize(loss)\n",
    "\n",
    "accuracy = tf.metrics.accuracy(          # return (acc, update_op), and create 2 local variables\n",
    "    labels=tf.argmax(tf_y, axis=1), predictions=tf.argmax(prediction, axis=1),)[1]\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer()) # the local var is for accuracy_op\n",
    "sess.run(init_op)     # initialize var in graph\n",
    "\n",
    "for step in range(10000):\n",
    "    batch = data.next_batch(BATCH_SIZE)\n",
    "    _, loss_ = sess.run([train_op, loss], {tf_x: batch[0], tf_y: batch[1], rate: 0.5 })\n",
    "#     sess.run(train_op,feed_dict={tf_x: batch_xs, tf_y: batch_ys})\n",
    "    if step % 50 == 0:\n",
    "#         print('Step:', step, '| train loss: %.4f' % loss_)\n",
    "        accuracy_ = sess.run(accuracy, {tf_x: data.test_images, tf_y: data.test_labels, rate: 1.0})\n",
    "        print('Step:', step, '| train loss: %.4f' % loss_, '| test accuracy: %.2f' % accuracy_)\n",
    "#         acc = sess.run(accuracy, feed_dict = {tf_x:mnist.test.images, tf_y:mnist.test.labels})\n",
    "#         print(acc)\n",
    "        #print('Step:', step, '| train loss: %.4f' % loss, '| test accuracy: %.2f' % accuracy)\n",
    "        \n",
    "        \n",
    "# print 10 predictions from test data\n",
    "test_output = sess.run(prediction, {tf_x: data.test_images[:20]})\n",
    "print('test_output', test_output.shape)\n",
    "pred_y = np.argmax(test_output, 1)\n",
    "print('pred_y', pred_y.shape)\n",
    "print(pred_y, 'prediction number')\n",
    "print(np.argmax(data.test_labels[:20], 1), 'real number')\n",
    "\n",
    "save_path = saver.save(sess, 'my_net/VerfiedCode_net.ckpt')\n",
    "print ('Model saved in file: ', save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
